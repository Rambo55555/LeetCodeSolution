[toc]



# Socket

##  一、IO模型

一个输入操作通常包括两个阶段：

- 等待数据准备好
- 从内核向进程复制数据

对于一个套接字上的输入操作，第一步通常涉及等待数据从网络中到达。当所等待数据到达时，它被复制到内核中的某个缓冲区。第二步就是把数据从内核缓冲区复制到应用进程缓冲区。

Unix 有五种 I/O 模型：

- 阻塞式 I/O
- 非阻塞式 I/O（轮询）
- I/O 复用（select 和 poll，事件驱动式）
- 信号驱动式 I/O（SIGIO）
- 异步 I/O（AIO）

### 阻塞式 I/O

应用进程被阻塞，直到数据从内核缓冲区复制到应用进程缓冲区中才返回。

应该注意到，在阻塞的过程中，其它应用进程还可以执行，因此阻塞不意味着整个操作系统都被阻塞。因为其它应用进程还可以执行，所以不消耗 CPU 时间，这种模型的 CPU 利用率会比较高。

下图中，recvfrom() 用于接收 Socket 传来的数据，并复制到应用进程的缓冲区 buf 中。这里把 recvfrom() 当成系统调用。

```
ssize_t recvfrom(int sockfd, void *buf, size_t len, int flags, struct sockaddr *src_addr, socklen_t *addrlen);
```

[![img](https://camo.githubusercontent.com/68a3f48b4948ac53d220664a56429c5e84c0667ff640d6038c6e9dced48da9e1/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f313439323932383431363831325f342e706e67)](https://camo.githubusercontent.com/68a3f48b4948ac53d220664a56429c5e84c0667ff640d6038c6e9dced48da9e1/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f313439323932383431363831325f342e706e67)

### 非阻塞式 I/O

应用进程执行系统调用之后，内核返回一个错误码。应用进程可以继续执行，但是需要不断的执行系统调用来获知 I/O 是否完成，这种方式称为轮询（polling）。

由于 CPU 要处理更多的系统调用，因此这种模型的 CPU 利用率比较低。

[![img](https://camo.githubusercontent.com/ecac1a2f263b198fa5660f7dd5a9accce515f49d6f7256a87fa1410049a1debe/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f313439323932393030303336315f352e706e67)](https://camo.githubusercontent.com/ecac1a2f263b198fa5660f7dd5a9accce515f49d6f7256a87fa1410049a1debe/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f313439323932393030303336315f352e706e67)

### I/O 复用

使用 select 或者 poll 等待数据，并且可以等待多个套接字中的任何一个变为可读。这一过程会被阻塞，当某一个套接字可读时返回，之后再使用 recvfrom 把数据从内核复制到进程中。

它可以让单个进程具有处理多个 I/O 事件的能力。又被称为 Event Driven I/O，即事件驱动 I/O。

如果一个 Web 服务器没有 I/O 复用，那么每一个 Socket 连接都需要创建一个线程去处理。如果同时有几万个连接，那么就需要创建相同数量的线程。相比于多进程和多线程技术，I/O 复用不需要进程线程创建和切换的开销，系统开销更小。

[![img](https://camo.githubusercontent.com/aa49b130631537882aa3d8338f3e8edd4136aca31ba5b8f84b7bd1c2a3049bc8/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f313439323932393434343831385f362e706e67)](https://camo.githubusercontent.com/aa49b130631537882aa3d8338f3e8edd4136aca31ba5b8f84b7bd1c2a3049bc8/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f313439323932393434343831385f362e706e67)

### 信号驱动 I/O

应用进程使用 sigaction 系统调用，内核立即返回，应用进程可以继续执行，也就是说等待数据阶段应用进程是非阻塞的。内核在数据到达时向应用进程发送 SIGIO 信号，应用进程收到之后在信号处理程序中调用 recvfrom 将数据从内核复制到应用进程中。

相比于非阻塞式 I/O 的轮询方式，信号驱动 I/O 的 CPU 利用率更高。

[![img](https://camo.githubusercontent.com/4ef6a2ee94e21ab9b8365a59a480fbbd44f375fd98da42873b5fcbb15530411e/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f313439323932393535333635315f372e706e67)](https://camo.githubusercontent.com/4ef6a2ee94e21ab9b8365a59a480fbbd44f375fd98da42873b5fcbb15530411e/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f313439323932393535333635315f372e706e67)

### 异步 I/O

应用进程执行 aio_read 系统调用会立即返回，应用进程可以继续执行，不会被阻塞，内核会在所有操作完成之后向应用进程发送信号。

异步 I/O 与信号驱动 I/O 的区别在于，异步 I/O 的信号是通知应用进程 I/O 完成，而信号驱动 I/O 的信号是通知应用进程可以开始 I/O。

[![img](https://camo.githubusercontent.com/4e523f16614b51e79c78bcf61234201ae4ca5b36b2bc4bb24572ee2622fe458c/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f313439323933303234333238365f382e706e67)](https://camo.githubusercontent.com/4e523f16614b51e79c78bcf61234201ae4ca5b36b2bc4bb24572ee2622fe458c/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f313439323933303234333238365f382e706e67)

### 五大 I/O 模型比较

- 同步 I/O：将数据从内核缓冲区复制到应用进程缓冲区的阶段（第二阶段），应用进程会阻塞。
- 异步 I/O：第二阶段应用进程不会阻塞。

同步 I/O 包括阻塞式 I/O、非阻塞式 I/O、I/O 复用和信号驱动 I/O ，它们的主要区别在第一个阶段。

非阻塞式 I/O 、信号驱动 I/O 和异步 I/O 在第一阶段不会阻塞。

![img](https://img2020.cnblogs.com/blog/1365470/202003/1365470-20200325090003059-662733553.png)

## IO复用

select、poll和epoll都是IO复用的具体实现。

### select

select将用户传入的需要监听的文件描述符数组拷贝到内核空间，然后查询每个fd对应的设备状态

**优点**

- 

**缺点**

- **最大连接数有限制** 

  因为select 的描述符类型使用数组实现，FD_SETSIZE 大小默认为 1024，因此默认只能监听少于 1024 个描述符。如果要监听更多描述符的话，需要修改 FD_SETSIZE 之后重新编译

- **对socket进行扫描时是线性扫描，即采用轮询的方法，效率较低，时间复杂度O(n)**

  当套接字比较多的时候，每次select()都要通过遍历FD_SETSIZE个Socket来完成调度,不管哪个Socket是活跃的,都遍历一遍。这会浪费很多CPU时间。如果能给套接字注册某个回调函数，当他们活跃时，自动完成相关操作，那就避免了轮询，这正是epoll与kqueue做的。

- **需要维护一个用来存放大量fd的数据结构，这样会使得用户空间和内核空间在传递该结构时复制开销大**

### poll

poll本质上和select没有区别，它将用户传入的数组拷贝到内核空间，然后查询每个fd对应的设备状态，如果设备就绪则在设备等待队列中加入一项并继续遍历，如果遍历完所有fd后没有发现就绪设备，则挂起当前进程，直到设备就绪或者主动超时，被唤醒后它又要再次遍历fd。

**优点**

- **没有最大连接数的限制，因为它的文件描述符实现方式为链表**

**缺点**

- 大量的fd的数组被整体复制于用户态和内核地址空间之间，而不管这样的复制是不是有意义。 
- **时间复杂度O(n)**
- 只有较新的系统支持poll



### epoll

Linux所特有的，向内核注册新的描述符或者是改变某个文件描述符的状态。已注册的描述符在内核中会被维护在一棵红黑树上，通过回调函数内核会将 I/O 准备好的描述符加入到一个链表中管理，进程调用 epoll_wait() 便可以得到事件完成的描述符。有两种模式：LT和ET。

LT：只要这个fd（文件描述符）还有数据可读，每次 epoll_wait都会返回它的事件，提醒用户程序去操作，也就是用户可以选择不处理，下次会再通知

ET：只会提示一次，直到下次再有数据流入之前都不会再提示了，无 论fd中是否还有数据可读。通知之后进程必须立即处理事件，下次不再通知

**优点**

- **epoll 只需要将描述符从进程缓冲区向内核缓冲区拷贝一次，并且进程不需要通过轮询来获得事件完成的描述符。**

- **没有最大连接数限制**

- **效率提升，不是轮询的方式，不会随着FD数目的增加效率下降。只有活跃可用的FD才会调用callback函数；**
  即Epoll最大的优点就在于它只管你“活跃”的连接，而跟连接总数无关，因此在实际的网络环境中，Epoll的效率就会远远高于select和poll。

- **复制开销减少，时间复杂度O(1)**

  内存拷贝，利用mmap()文件映射内存加速与内核空间的消息传递；即epoll使用mmap减少复制开销。就是不用拷贝了，因为mmap保存了映射地址，相当于共享内存

**缺点**

- **表面上看epoll的性能最好，但是在连接数少并且连接都十分活跃的情况下，select和poll的性能可能比epoll好，毕竟epoll的通知机制需要很多函数回调**
- **只适用于Linux**